{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d949908-9d31-40bd-9a0b-a46fd40ac779",
   "metadata": {},
   "source": [
    "# ELEC-E5500 Speech Processing -- Autumn 2025\n",
    "## Exercise 4: Wake Word Detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc6b30-403a-4afc-8f99-9696a65e01d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview:\n",
    "\n",
    "**Implement and fill in this exercise, you will implement and train your own custom wake word detector**.\n",
    "\n",
    "Submit the notebook by 23:59 on **29.09.24** (Only one person of you needs to submit this notebook.)\n",
    "\n",
    "Do this together as a group, and choose a custom wake word. Often something with at least with 3 syllables works best (as the false positives rate is greatly reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd72b88-c403-40b1-bf8f-73ada2e54f05",
   "metadata": {},
   "source": [
    "## Write the name of the people in the team here! TEAM MEMBERS: \n",
    "\n",
    "* Khanh Ha\n",
    "* Enikő Palencsár\n",
    "* Angelo Parravano\n",
    "* Jere Tahvanainen\n",
    "* Tuan Tran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478d108-41fa-49dc-9217-79756d7a1926",
   "metadata": {},
   "source": [
    "Since the jupyter here, consistently breaks, when running the training, it is much easier to run the wake word detection model on google colab. \n",
    "The demo we are using today, is this: https://colab.research.google.com/drive/1q1oe2zOyZp7UsB3jJiQ1IFn8z5YfjwEb?usp=sharing#scrollTo=1cbqBebHXjFD\n",
    "\n",
    "Change the target_word to whatever you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8424eb-d79d-4eef-82d1-844c0ded1bfb",
   "metadata": {},
   "source": [
    "After training the wakeword detection model with the google colab (it should take a couple of hours on the free plan) \n",
    "\n",
    "1. Upload a model of yours to your jupyter folder (the tflite, and the onnx file) which will be submitted\n",
    "\n",
    "2. Write your target_word in the next cell (both, how it was supposed to sound like, and what actual string you used in the target word)\n",
    "\n",
    "3. Explain how the model is trained, what datasets are used, what model is used, and what parameters can you change?\n",
    "\n",
    "4. Start changing at least 3 things in the training, and see what effect it has on the final output: Here are some ideas (what happens when you increase the number of samples, what happens when you change the negative sample penalization or what happens when leave out some of the augmentations? (this are ideas, you can also choose other things) - write about the effects in the next cell.\n",
    "\n",
    "5. Find a new dataset, that could be used for data augmentation (for example huggingface is a good place to start)\n",
    "\n",
    "6. Write about strategies how you could improve the performance of the model (that could be finding different architecturs, losses, training data, real recordings, elaborate as concretely as you can)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46cd69-1efa-46c5-9ebf-8f47b1a64027",
   "metadata": {},
   "source": [
    "Have each member of your group record saying the wake word you chose, and upload it to jupyter.\n",
    "\n",
    "Then run the codecell below to see what your model would predict.\n",
    "\n",
    "7. How does the model perform with real life recordings?\n",
    "8. Can you find a false positive sample, that consistently activates the wake word, without actually being the wake word? - Mention what words you tried and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d9f2e-4cbb-413a-927a-5566e7aac8e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we need to install openwakeword\n",
    "!pip install openwakeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60cfe0-5740-4614-a378-bb5c7829ce47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predictions for individual WAV files (16-bit 16khz PCM)\n",
    "import openwakeword\n",
    "from openwakeword.model import Model\n",
    "\n",
    "# This might be helpful and downloads some pretrained mdoels: openwakeword.utils.download_models()\n",
    "# The file can either be the tflite or onnx file (more information can be found here https://github.com/dscripka/openWakeWord/blob/main/openwakeword/model.py. \n",
    "\n",
    "model = Model(wakeword_models=[\"path/to/model.tflite\"])\n",
    "model.predict_clip(\"path/to/wavfile\")\n",
    "\n",
    "# The prediction will be on a frame by frame basis (80ms at a time for example), you can either use the average of the\n",
    "#predicitions or see if there are certains frames activated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8549f-de66-4e27-a7a3-74c55c82468a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e950c457375d5f13a596a4b90d9bba19",
     "grade": true,
     "grade_id": "cell-b55f48256622e7fb",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 2. Write your target_word in the next cell (both, how it was supposed to sound like, and what actual string you used in the target word)\n",
    "\n",
    "**target:** activate assistant /ˈæk.təˌveɪt əˈsɪs.tənt/ \\\n",
    "**actual string used:** activate assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76377c1a",
   "metadata": {},
   "source": [
    "### 3. Explain how the model is trained, what datasets are used, what model is used, and what parameters can you change?\n",
    "\n",
    "#### Model and training\n",
    "\n",
    "The *openWakeWord* model consists of three separate parts: pre-processing, feature extraction and classification. As features, the mel-spectrograms are computed from the input audio data which should be 16-bit 16kHz PCM data. In the mel-spectrogram, frequency bands are created in a way that reflects how humans are sensitive to them, so higher frequencies are compressed more. After the pre-processing, the feature extraction layers allow for the conversion of melspectrograms to audio embeddings. This feature extraction component of the model, based on simple convolutional blocks, is a reimplementation of a TFHub module. Its strength lies in it being pre-trained on a large data set. During the training of the wakeword spotter, these convolutional layers are frozen, and only the last part of the model, the classification network learns weights. This refers to a fully-connected feedforward neural network in our case, with ReLu activation functions used.\n",
    "\n",
    "For training the model, the Adam optimizer is used. The learning rate for training has a specific schedule that starts with a warmup phrase (1/5 of total steps), where it increases in a linear fashion. After that, it can hold for a set number of steps (1/3 of total steps) and then decays toward zero following the cosine curve. The training also involves hard example mining, filtering the training batch to focus on difficult samples – negative samples for which the predicted values are too high and positive samples for which the predictions are too low. \n",
    "\n",
    "There are 3 separate sequences of training, the second and third with only 10% of the total steps but increased weights of negative examples to reduce the false positive rate. The resulting models are merged in a way that models above the 90th percentile are considered and their weights are averaged. In each training step, the resulting model is saved only if it meets certain performance criteria relative to the previously saved models (low false positive rate, high recall).\n",
    "\n",
    "---\n",
    "\n",
    "#### Datasets - training\n",
    "\n",
    "The model uses synthetically created positive samples to train for wakeword detection. Sample generation is carried out using a text-to-speech tool called *Piper Sample Generator*. This also involves for example using multiple speaker voices and introducing variability into the generated speech. The negative samples for training are drawn from the *ACAV100M* dataset. Their features have been pre-computed to act as general purpose negative training data in *openWakeWord*, speeding up the training process. The false-positive validation set contains approximately 11 hours of audio including the *DiPCo* dataset, the *Santa Barbara Corpus of Spoken American English* and some clips from the *MUSDB Music Dataset*, reverberated using the *MIT impulse response recordings*.\n",
    "\n",
    "**ACAV100M**\n",
    "\n",
    "*ACAV100M* is an automatically curated dataset containing 31 years worth of 10-second-long clips of speech, noise and music in multiple languages, retrieved from YouTube.\n",
    "\n",
    "**DiPCo**\n",
    "\n",
    "The *Dinner Party Corpus* was created with the assistance of volunteers, who simulated a dinner-party scenario in a lab.\n",
    "\n",
    "**Santa Barbara Corpus of Spoken American English**\n",
    "\n",
    "This dataset is a collection of naturally occurring spoken interactions gathered from across the United States. It includes speech from individuals representing diverse regional backgrounds, ages, occupations, genders and ethnicities.\n",
    "\n",
    "**MUSDB Music Dataset**\n",
    "\n",
    "*MUSDB* is a dataset of 150 music tracks of different genres.\n",
    "\n",
    "---\n",
    "\n",
    "#### Datasets - augmentation\n",
    "\n",
    "Data augmentation is also included before the training, including adding coloured noise, real-world background noise, music, and echo. There are also equalizer, distortion, pitch shift, volume change and frequency band removal effects, all added or not with a given probability.\n",
    "\n",
    "**MIT environmental impulse responses**\n",
    "\n",
    "The dataset, comprising 271 clips, was recorded by the Computational Audition Lab at MIT. These audio files contain diverse environmental impulse response data. It’s used to add echo to the clips in the training process.\n",
    "\n",
    "**AudioSet dataset**\n",
    "\n",
    "This is a dataset of 10-second clips from YouTube, a subset of which is used for adding background noise to our examples. It contains clips under 527 labels such as traffic noise, wind noise, environmental noise but also yodeling, speech, whistle, purr and ocean sounds.\n",
    "\n",
    "**Free Music Archive**\n",
    "\n",
    "Contains 30-second clips of music of which a total of 1 hour is used in the training process to add background music to some audio samples.\n",
    "\n",
    "---\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "Multiple parameters of the model and the training process can be adjusted. The base model with the default parameter values (given in brackets) had a training time of about 1 hour.\n",
    "\n",
    "A non-exhaustive list of model parameters:\n",
    "1.\t**model type**: can be a simple feedforward deep neural network or a recurrent neural network with an LSTM layer *(DNN)*\n",
    "2.\t**layer size**: only for DNN, RNN parameters are fixed *(32)*\n",
    "3.\t**number of hidden layers**: only for DNN, RNN parameters are fixed *(1)*\n",
    "\n",
    "A non-exhaustive list of training parameters:\n",
    "1.\t**target phrase** which the model (hopefully) learns to spot *(activate assistant)*\n",
    "2.\tcustom **negative phrases** that are easy to mistake for the target phrase *(empty [])*\n",
    "    * there are negative phrases deliberately generated during the training process using phoneme overlap, but if certain similar expressions still cause the model to fail, these can be included here\n",
    "3.\t**number of positive samples** generated and used for training the model *(1000)*, also the number of positive samples generated for validation and early stopping checks *(max(500, number_of_examples//10) = 500)*\n",
    "4.\tthe maximum number of **training steps** *(10 000)*\n",
    "5.\ttarget **accuracy** *(0.5)* and **recall** *(0.25)*\n",
    "6.\t**penalty for false activation** which limits how much influence negative samples can have during training *(1500)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b5bf2",
   "metadata": {},
   "source": [
    "### 5. Find a new dataset, that could be used for data augmentation (for example huggingface is a good place to start)\n",
    "\n",
    "#### Arni\n",
    "\n",
    "For room impulse responses, the current configuration uses an MIT dataset of 271 samples from 2016. There are however many more extensive and more recent (therefore recorded with better equipment) datasets. One of such datasets was collected in the variable acoustics laboratory Arni at Aalto. It comprises 132 037 RIRs measured using 5342 configurations of 55 acoustic panels in the lab.\n",
    "\n",
    "[Link to dataset](https://zenodo.org/records/6985104#.YwffZuzMIeY)\n",
    "\n",
    "#### FSD50k\n",
    "\n",
    "FS50K is a dataset of more than 50 000 samples which equals more than 100 hours of audio. The sounds in the clips are mostly human sounds, sounds of things, animal sounds, natural sounds and music. All clips are provided as uncompressed PCM 16 bit 44.1 kHz mono audio files, so the sampling frequencies should be adjusted before using the audio as background noise in the enhancement phase.\n",
    "\n",
    "[Link to dataset](https://huggingface.co/datasets/Fhrozen/FSD50k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77226a50",
   "metadata": {},
   "source": [
    "### 6. Strategies for Improvement\n",
    "\n",
    "#### Training data\n",
    "\n",
    "The most effective way to improve the model is to address the quality and balance of the training data. At present, the positives are generated synthetically with Piper TTS. This helps bootstrap the model but creates a clear mismatch to real deployment conditions. To close this gap, at least 15–20 speakers should record the target phrase *“activate assistant”*, each providing 20–30 samples in different environments and microphones. These recordings can be included directly in the `positive` class. To reduce confusions, more near-miss negatives should be added to `custom_negative_phrases` (e.g. *“assistant activate”*, *“activate system”*, *“hey assistant”*). The dataset is also heavily skewed: the YAML shows `batch_n_per_class` with only 50 positives versus 1024 negatives from ACAV100M. This imbalance will depress recall. Either raise the positive batch size to 200+ or reduce the effect of negatives by lowering `max_negative_weight`. Background coverage should also be extended beyond FMA and AudioSet to include more everyday sounds (office chatter, TV, kitchen noise). Finally, augmentations should be made stronger: increase `augmentation_rounds` from 1 to 3–5 and add pitch shifting (±2 semitones), time stretching (0.9–1.1×), SNR mixes between 0 and 20 dB, and a larger set of room impulse responses. The Arni dataset (132k RIRs) is a promising replacement for the older MIT RIR set, and additional noise datasets like FSD50k (100 hours of diverse sounds) could improve robustness.\n",
    "\n",
    "#### Training procedure\n",
    "\n",
    "The YAML currently specifies `steps=25000`, which is too low once the data is expanded. Extending to 50k–100k steps ensures the model fully benefits from the richer dataset. The validation set is also too small (`n_samples_val=500`), and should be increased to a few thousand samples, with near-misses included, so that false positive rates can be tracked more reliably. A curriculum approach is also recommended: begin training with clean TTS positives and low-noise data, then gradually add noisy, reverberant, and accented recordings. This staged exposure prevents the model from overfitting early to easy cases while still ensuring robustness later.\n",
    "\n",
    "#### Loss functions\n",
    "\n",
    "The current setup relies on binary cross-entropy. Switching to focal loss (γ≈2, α≈0.25) would force the model to focus more on the hard negatives that dominate the dataset. An alternative is to add an embedding layer before the classifier and use triplet or contrastive loss with near-miss phrases. This explicitly separates them from the true wake word in the embedding space. In either case, class weighting should be applied so that positives have enough influence during training despite their smaller numbers.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "The classifier in the YAML is a simple feed-forward DNN with `layer_size=32`. This capacity is limited given the variability in speakers, microphones, and noise conditions. Increasing to 64–128 units per layer will improve feature representation. More importantly, the model should not remain fully feed-forward: temporal modeling should be introduced by reshaping the mel-spectrogram features into sequences and feeding them into a GRU or LSTM layer (e.g. 64 units), or by using a temporal CNN block. This allows the system to capture timing variations in how the phrase *“activate assistant”* is spoken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bda471",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
